# publisher.py
import asyncio
import redis.asyncio as aioredis
import uuid
import json
from datetime import datetime, timezone
import redis.exceptions
import logging

# Asumiendo que partitioner.py est√° en el mismo nivel o en el PYTHONPATH
from .partitioner import get_partition
# from .config import PARTITIONS # No se necesita si se pasa en __init__

logger = logging.getLogger(__name__)

class Publisher:
    """
    Publishes jobs to specific partitions of a Redis stream based on a partition key.
    Designed to be instantiated once and used across the application.
    """
    def __init__(self, redis_url: str, total_partitions: int):
        """
        Initializes the Publisher.

        Args:
            redis_url: The connection URL for the Redis server.
            total_partitions: The total number of partitions configured for the system.
        """
        if total_partitions <= 0:
            raise ValueError("total_partitions must be greater than zero")

        self.redis_url = redis_url
        self.total_partitions = total_partitions
        self.redis = None
        self._connected = False
        logger.info(f"Publisher initialized for Redis at {redis_url} with {total_partitions} total partitions.")

    async def connect(self):
        """Establishes connection to Redis."""
        if self._connected:
            logger.debug("Publisher already connected.")
            return
        try:
            self.redis = await aioredis.from_url(self.redis_url, decode_responses=True)
            await self.redis.ping()
            self._connected = True
            logger.info(f"Publisher connected successfully to Redis at {self.redis_url}.")
        except (redis.exceptions.ConnectionError, OSError) as e:
            logger.error(f"Publisher connection failed: {e}")
            self.redis = None
            self._connected = False
            raise

    async def publish_job(
        self, stream_name: str, data: dict, partition_key: str | int | None = None
    ) -> str | None:
        """
        Publishes a job to the appropriate partition of the specified stream.

        Args:
            stream_name: The base name of the stream to publish to (e.g., "orders").
            data: The job data dictionary. Should contain relevant info, including potentially
                  a 'cmd' field for the consumer handler.
            partition_key: The key used to determine the partition. If None, it tries
                           to use 'partition_id' or 'to' from the data, falling back to
                           a new job_id.

        Returns:
            The job_id if successfully published, None otherwise.
        """
        if not self._connected or not self.redis:
            logger.error(f"Cannot publish job to '{stream_name}': Publisher not connected.")
            return None
        if not stream_name:
             logger.error("Cannot publish job: stream_name is empty.")
             return None

        job_id = str(uuid.uuid4())
        # Ensure standard fields are present
        payload = {
            "job_id": job_id,
            "published_at": datetime.now(timezone.utc).isoformat(),
            **data, # Include original data
        }

        # Determine partition key more explicitly
        key_source_val = None
        key_source_name = "none" # For logging clarity

        if partition_key is not None:
             key_source_val = partition_key
             key_source_name = "explicit"
        elif "partition_id" in data:
             key_source_val = data["partition_id"]
             key_source_name = "data[partition_id]"
        elif "to" in data: # Example: user_id, device_id etc.
             key_source_val = data["to"]
             key_source_name = "data[to]"
        else:
             key_source_val = job_id # Fallback to job_id for distribution
             key_source_name = "job_id (fallback)"

        # Calculate partition using the chosen key
        partition = get_partition(key_source_val, self.total_partitions)
        stream_partition_name = f"{stream_name}:{partition}"

        try:
            # Serialize payload - ensure it's JSON serializable
            job_json = json.dumps(payload)

            # Add job to the specific stream partition
            # XADD stream_key * field value [field value ...]
            message_id = await self.redis.xadd(stream_partition_name, {"job": job_json})
            logger.info(
                f"Published job {job_id} (MsgID: {message_id}) to stream '{stream_partition_name}' "
                f"(Partition {partition} of {self.total_partitions}, Key: '{key_source_name}'='{key_source_val}')"
            )
            return job_id # Return the job_id generated by the publisher

        except redis.exceptions.RedisError as e:
            logger.error(f"Redis error publishing job {job_id} to {stream_partition_name}: {e}")
            return None
        except TypeError as e:
            # Error during json.dumps
            logger.error(f"Failed to serialize job data for job {job_id} (Stream: {stream_name}): {e}")
            return None
        except Exception as e:
             logger.exception(f"Unexpected error publishing job {job_id} to {stream_partition_name}: {e}")
             return None

    async def close(self):
        """Closes the Redis connection."""
        if self.redis:
            try:
                await self.redis.close()
                logger.info("Publisher connection closed.")
            except redis.exceptions.RedisError as e:
                logger.error(f"Error closing Publisher connection: {e}")
            finally:
                self.redis = None
                self._connected = False
        else:
             logger.info("Publisher connection already closed or not established.")